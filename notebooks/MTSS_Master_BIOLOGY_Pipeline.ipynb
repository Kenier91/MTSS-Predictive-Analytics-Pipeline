{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MTSS Master Pipeline (No Low-25 Tracker)\n",
        "\n",
        "This reusable notebook processes MTSS assessment data for subjects **like Biology** that do not utilize a Low-25 tracking list. Update the Configuration Block below with your specific school name, subject name, and filenames. The pipeline handles missing data via median imputation, calculates growth, clusters students, exports formatted Excel packets to an `OUTPUTS` folder, and runs a **Comparative Statistical Analysis** across teachers."
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libs"
      },
      "outputs": [],
      "source": [
        "# 0. Install necessary libraries for PDF generation and stats\n",
        "!pip install -q fpdf statsmodels scikit-posthocs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import itertools\n",
        "from sklearn.metrics import silhouette_score\n",
        "import openpyxl\n",
        "from openpyxl.styles import PatternFill, Font, Alignment\n",
        "import scipy.stats as stats\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from fpdf import FPDF\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION BLOCK & Setup\n",
        "# ==========================================\n",
        "SCHOOL_NAME = \"School A\"      # e.g., \"BHS\", \"CHS\"\n",
        "SUBJECT_NAME = \"Biology\"  # e.g., \"Biology\", \"US History\"\n",
        "\n",
        "# Update these exact filenames to match what you uploaded to Colab:\n",
        "FILE_MAIN_DATA = \"BBCard_Bio_25-26.csv\"\n",
        "FILE_ATTENDANCE = \"Bio_Attendance_25-26.csv\"\n",
        "\n",
        "output_dir = \"OUTPUTS\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "main_df = pd.read_csv(FILE_MAIN_DATA)\n",
        "attendance_df = pd.read_csv(FILE_ATTENDANCE)\n",
        "# Note: Low 25 data processing removed for this subject pipeline."
      ],
      "metadata": {
        "id": "config_and_load"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. Prepare & Merge Attendance Data\n",
        "# ==========================================\n",
        "attendance_melt = attendance_df.melt(id_vars=['Student ID', 'Last First M'],\n",
        "                                     value_vars=[c for c in attendance_df.columns if 'PERIOD' in c],\n",
        "                                     var_name='Period_Raw', value_name='Teacher_Room')\n",
        "\n",
        "def extract_teacher_last_name(tr_str):\n",
        "    if pd.isna(tr_str): return \"\"\n",
        "    parts = str(tr_str).split(',')\n",
        "    if len(parts) > 0: return parts[0].strip().upper()\n",
        "    return \"\"\n",
        "\n",
        "attendance_melt['Teacher_Last'] = attendance_melt['Teacher_Room'].apply(extract_teacher_last_name)\n",
        "main_df['Teacher_Last'] = main_df['Teacher'].astype(str).apply(lambda x: x.split(',')[0].strip().upper() if ',' in x else x.strip().upper())\n",
        "\n",
        "merged = pd.merge(main_df, attendance_melt, how='left',\n",
        "                  left_on=['Student Id', 'Teacher_Last'],\n",
        "                  right_on=['Student ID', 'Teacher_Last'])\n",
        "merged = merged.sort_values('Period_Raw').drop_duplicates(subset=['Student Id'], keep='first')"
      ],
      "metadata": {
        "id": "merge_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In cell 3. # ADD 'Current Grade' if in"
      ],
      "metadata": {
        "id": "veGKhHVqhDnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. Clean, Format, and Impute Data (MEDIAN)\n",
        "# ==========================================\n",
        "test_cols = [c for c in merged.columns if 'Test Percentage Score' in c and 'USA' in c]\n",
        "test_cols.sort()\n",
        "std_cols = [c for c in merged.columns if 'Standards Percentage Score' in c]\n",
        "\n",
        "for col in test_cols + std_cols:\n",
        "    merged[col] = pd.to_numeric(merged[col], errors='coerce')\n",
        "    col_median = merged[col].median()\n",
        "    merged[col] = merged[col].fillna(col_median)\n",
        "\n",
        "out_df = merged[['Teacher', 'Student Id', 'Student Name', 'Period_Raw',\n",
        "                 'Gender', 'SWD/ESE', 'ELL', '504', 'Ethnicity']].copy() # ADD 'Current Grade' if in\n",
        "\n",
        "def format_period(p):\n",
        "    if pd.isna(p): return \"Unknown Period\"\n",
        "    match = re.search(r'PERIOD\\s*0?(\\d+)', str(p), re.IGNORECASE)\n",
        "    if match: return f\"P{match.group(1)}\"\n",
        "    return str(p)\n",
        "\n",
        "out_df['Period'] = out_df['Period_Raw'].apply(format_period)\n",
        "out_df.drop(columns=['Period_Raw'], inplace=True)\n",
        "\n",
        "# Clean categorical columns (Low_25 removed)\n",
        "cat_cols = ['Gender', 'SWD/ESE', 'ELL', '504', 'Ethnicity', 'Teacher'] # ADD 'Current Grade' if in\n",
        "for col in cat_cols:\n",
        "    out_df[col] = out_df[col].astype(str).fillna('Unknown')"
      ],
      "metadata": {
        "id": "clean_impute"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. Calculate Growth and Assign Tiers\n",
        "# ==========================================\n",
        "def calc_sequential_growth(row):\n",
        "    scores = pd.to_numeric(row[test_cols], errors='coerce').dropna().values\n",
        "    if len(scores) < 2: return np.nan\n",
        "    return np.mean(np.diff(scores))\n",
        "\n",
        "out_df['Average_USA'] = merged[test_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1).fillna(0)\n",
        "out_df['Growth'] = merged.apply(calc_sequential_growth, axis=1)\n",
        "out_df['Above_Benchmark'] = out_df['Average_USA'].apply(lambda x: 1 if x >= 60 else 0)\n",
        "\n",
        "def growth_category(growth_value):\n",
        "    if pd.isna(growth_value): return \"Insufficient Data\"\n",
        "    elif growth_value >= 5: return \"Strong\"\n",
        "    elif growth_value >= 2: return \"Moderate\"\n",
        "    elif growth_value >= 0: return \"Weak\"\n",
        "    else: return \"Negative Growth\"\n",
        "\n",
        "out_df['Growth_Level'] = out_df['Growth'].apply(growth_category)\n",
        "\n",
        "def assign_tier(row):\n",
        "    ab = row[\"Above_Benchmark\"]\n",
        "    gl = row[\"Growth_Level\"]\n",
        "    if ab == 1:\n",
        "        return \"Tier 1\" if gl == \"Strong\" else \"Tier 1 - Monitoring\"\n",
        "    else:\n",
        "        return \"Tier 2\" if gl in [\"Moderate\", \"Strong\"] else \"Tier 3\"\n",
        "\n",
        "out_df['Tier'] = out_df.apply(assign_tier, axis=1)\n",
        "\n",
        "def get_bad_standards(row):\n",
        "    bad = []\n",
        "    for c in std_cols:\n",
        "        val = pd.to_numeric(row[c], errors='coerce')\n",
        "        if pd.notna(val) and val < 60:\n",
        "            std_name = c.split('\\n')[-1]\n",
        "            if std_name not in bad: bad.append(std_name)\n",
        "    return \", \".join(bad)\n",
        "\n",
        "out_df['Standards_Needing_Intervention'] = merged.apply(get_bad_standards, axis=1)\n",
        "out_df['Average_USA'] = out_df['Average_USA'].round(1)\n",
        "out_df['Growth'] = out_df['Growth'].round(1)\n",
        "out_df['Cluster'] = \"\"\n"
      ],
      "metadata": {
        "id": "growth_tiers"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In cell 5. # ADD 'Current Grade' if in"
      ],
      "metadata": {
        "id": "_VYgypCRg4K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. K-Prototypes Clustering\n",
        "# ==========================================\n",
        "def compute_distance_matrix(num_data, cat_data, gamma=1.5):\n",
        "    N = num_data.shape[0]\n",
        "    std_num = (num_data - np.mean(num_data, axis=0)) / (np.std(num_data, axis=0) + 1e-8)\n",
        "    dist_matrix = np.zeros((N, N))\n",
        "    for i in range(N):\n",
        "        dist_num = np.sum((std_num - std_num[i])**2, axis=1)\n",
        "        dist_cat = np.sum(cat_data != cat_data[i], axis=1)\n",
        "        dist_matrix[i, :] = dist_num + gamma * dist_cat\n",
        "    return dist_matrix\n",
        "\n",
        "def k_prototypes_simple(num_data, cat_data, k=3, max_iter=100, gamma=1.5):\n",
        "    np.random.seed(42)\n",
        "    N = num_data.shape[0]\n",
        "    if N < k: k = N\n",
        "    if k == 1: return np.ones(N)\n",
        "    std_num = (num_data - np.mean(num_data, axis=0)) / (np.std(num_data, axis=0) + 1e-8)\n",
        "    init_idx = np.random.choice(N, k, replace=False)\n",
        "    num_centroids = std_num[init_idx].copy()\n",
        "    cat_centroids = cat_data[init_idx].copy()\n",
        "    clusters = np.zeros(N)\n",
        "    for _ in range(max_iter):\n",
        "        new_clusters = np.zeros(N)\n",
        "        for i in range(N):\n",
        "            dist_num = np.sum((num_centroids - std_num[i])**2, axis=1)\n",
        "            dist_cat = np.sum(cat_centroids != cat_data[i], axis=1)\n",
        "            new_clusters[i] = np.argmin(dist_num + gamma * dist_cat)\n",
        "        if np.array_equal(clusters, new_clusters): break\n",
        "        clusters = new_clusters\n",
        "        for c in range(k):\n",
        "            mask = (clusters == c)\n",
        "            if np.any(mask):\n",
        "                num_centroids[c] = np.mean(std_num[mask], axis=0)\n",
        "                for j in range(cat_data.shape[1]):\n",
        "                    vals, counts = np.unique(cat_data[mask, j], return_counts=True)\n",
        "                    cat_centroids[c, j] = vals[np.argmax(counts)]\n",
        "    return clusters + 1\n",
        "\n",
        "for tier in out_df['Tier'].unique():\n",
        "    tier_mask = out_df['Tier'] == tier\n",
        "    tier_df = out_df[tier_mask]\n",
        "    if len(tier_df) < 3:\n",
        "        out_df.loc[tier_mask, 'Cluster'] = 1\n",
        "        continue\n",
        "    num_features = tier_df[['Average_USA', 'Growth']].copy()\n",
        "    num_features['Growth'] = num_features['Growth'].fillna(0)\n",
        "    num_features = num_features.values\n",
        "    cat_features = tier_df[cat_cols].values\n",
        "    dist_mat = compute_distance_matrix(num_features, cat_features, gamma=1.5)\n",
        "    best_k, best_score, best_labels = 2, -1, None\n",
        "    max_k_test = min(5, len(tier_df) - 1)\n",
        "    if max_k_test >= 2:\n",
        "        for k in range(2, max_k_test + 1):\n",
        "            labels = k_prototypes_simple(num_features, cat_features, k=k)\n",
        "            if len(np.unique(labels)) < 2: continue\n",
        "            try: score = silhouette_score(dist_mat, labels, metric='precomputed')\n",
        "            except ValueError: score = -1\n",
        "            if score > best_score:\n",
        "                best_score, best_k, best_labels = score, k, labels\n",
        "    else:\n",
        "        best_k = len(tier_df)\n",
        "        best_labels = k_prototypes_simple(num_features, cat_features, k=best_k)\n",
        "\n",
        "    if best_labels is not None:\n",
        "        tier_df = tier_df.copy()\n",
        "        tier_df['temp_label'] = best_labels\n",
        "        means = tier_df.groupby('temp_label')['Average_USA'].mean().sort_values(ascending=False)\n",
        "        mapping = {old_lbl: new_lbl+1 for new_lbl, old_lbl in enumerate(means.index)}\n",
        "        out_df.loc[tier_mask, 'Cluster'] = tier_df['temp_label'].map(mapping)\n",
        "    else:\n",
        "        out_df.loc[tier_mask, 'Cluster'] = 1\n",
        "\n",
        "cols_order = ['Teacher', 'Student Id', 'Student Name', 'Period',\n",
        "              'Gender', 'SWD/ESE', 'ELL', '504', 'Ethnicity', 'Average_USA', 'Growth',\n",
        "              'Above_Benchmark', 'Growth_Level', 'Tier', 'Cluster', 'Standards_Needing_Intervention']   # ADD 'Current Grade' if in\n",
        "out_df = out_df[cols_order]"
      ],
      "metadata": {
        "id": "clustering"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. Generate Tier Summaries (Helper Function)\n",
        "# ==========================================\n",
        "def generate_tier_distribution(df):\n",
        "    total_students = len(df)\n",
        "    tier_counts = df['Tier'].value_counts()\n",
        "    order = [\"Tier 1\", \"Tier 1 - Monitoring\", \"Tier 2\", \"Tier 3\"]\n",
        "    dist_data = []\n",
        "    for t in order:\n",
        "        count = tier_counts.get(t, 0)\n",
        "        percent = f\"{(count / total_students) * 100:.1f}%\" if total_students > 0 else \"0.0%\"\n",
        "        if t in [\"Tier 1\", \"Tier 1 - Monitoring\"]: note = \"Clustered for instructional pattern analysis\"\n",
        "        elif t == \"Tier 2\": note = \"Clustered for targeted intervention planning\"\n",
        "        elif t == \"Tier 3\": note = \"Clustered for intensive intervention planning\"\n",
        "        else: note = \"\"\n",
        "        dist_data.append({\"TIER\": t, \"# STUDENTS\": count, \"CLUSTERED?\": \"Yes\" if count > 0 else \"No\", \"NOTES\": note, \"PERCENT\": percent})\n",
        "    dist_data.append({\"TIER\": \"Total\", \"# STUDENTS\": total_students, \"CLUSTERED?\": \"\", \"NOTES\": \"\", \"PERCENT\": \"100.0%\" if total_students > 0 else \"0.0%\"})\n",
        "    return pd.DataFrame(dist_data)\n",
        "\n",
        "overall_dist_df = generate_tier_distribution(out_df)\n",
        "master_dist_sheet_name = f\"{SCHOOL_NAME} Tier Distribution\"\n",
        "teacher_dist_sheet_name = \"Class Tier Distribution\""
      ],
      "metadata": {
        "id": "tier_summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. Export Workbooks (Master & Individual)\n",
        "# ==========================================\n",
        "csv_path = os.path.join(output_dir, f\"MTSS_{SUBJECT_NAME}.csv\")\n",
        "excel_path = os.path.join(output_dir, f\"MTSS_{SUBJECT_NAME}.xlsx\")\n",
        "out_df.to_csv(csv_path, index=False)\n",
        "\n",
        "def format_worksheet(ws, df_cols):\n",
        "    ws.freeze_panes = 'A2'\n",
        "    ws.auto_filter.ref = ws.dimensions\n",
        "    header_fill = PatternFill(start_color=\"FFA500\", end_color=\"FFA500\", fill_type=\"solid\")\n",
        "    header_font = Font(bold=True)\n",
        "    wrap_alignment = Alignment(wrap_text=True, vertical='top')\n",
        "    no_wrap_alignment = Alignment(wrap_text=False, vertical='top')\n",
        "    for cell in ws[1]:\n",
        "        cell.fill = header_fill\n",
        "        cell.font = header_font\n",
        "        cell.alignment = no_wrap_alignment\n",
        "\n",
        "    # Format specifically for Tier Distribution summary sheets\n",
        "    if 'Tier Distribution' in ws.title:\n",
        "        for col in ws.columns:\n",
        "            ws.column_dimensions[col[0].column_letter].width = 25\n",
        "        ws.column_dimensions['D'].width = 50\n",
        "    else:\n",
        "        std_col_idx = df_cols.index('Standards_Needing_Intervention') + 1 if 'Standards_Needing_Intervention' in df_cols else None\n",
        "        for row in ws.iter_rows(min_row=2):\n",
        "            for i, cell in enumerate(row, start=1):\n",
        "                if i == std_col_idx:\n",
        "                    cell.alignment = wrap_alignment\n",
        "                else:\n",
        "                    cell.alignment = no_wrap_alignment\n",
        "        for col in ws.columns:\n",
        "            column_letter = col[0].column_letter\n",
        "            header_val = str(col[0].value)\n",
        "            if header_val == 'Standards_Needing_Intervention': ws.column_dimensions[column_letter].width = 50\n",
        "            elif header_val in ['Average_USA', 'Growth', 'Above_Benchmark', 'Cluster', 'Period']: ws.column_dimensions[column_letter].width = 15\n",
        "            else:\n",
        "                max_length = max([len(str(cell.value)) for cell in col if cell.value is not None] + [0])\n",
        "                ws.column_dimensions[column_letter].width = min(max_length + 2, 35)\n",
        "\n",
        "teachers = [t for t in out_df['Teacher'].unique() if str(t).strip() != '' and str(t).lower() != 'nan']\n",
        "\n",
        "# 1. Export Master Workbook (Uses School Name for Tier Tab)\n",
        "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "    out_df.to_excel(writer, sheet_name='OVERALL', index=False)\n",
        "    overall_dist_df.to_excel(writer, sheet_name=master_dist_sheet_name, index=False)\n",
        "    for t in teachers:\n",
        "        t_name = str(t)[:31]\n",
        "        for char in [':', '/', '\\\\', '?', '*', '[', ']']: t_name = t_name.replace(char, '')\n",
        "        t_df = out_df[out_df['Teacher'] == t]\n",
        "        if not t_df.empty: t_df.to_excel(writer, sheet_name=t_name, index=False)\n",
        "\n",
        "wb = openpyxl.load_workbook(excel_path)\n",
        "for sheet_name in wb.sheetnames:\n",
        "    cols = list(overall_dist_df.columns) if sheet_name == master_dist_sheet_name else list(out_df.columns)\n",
        "    format_worksheet(wb[sheet_name], cols)\n",
        "wb.save(excel_path)\n",
        "\n",
        "# 2. Export Individual Teacher Workbooks (Uses \"Class Tier Distribution\" for Tier Tab)\n",
        "for t in teachers:\n",
        "    t_name = str(t)[:31]\n",
        "    for char in [':', '/', '\\\\', '?', '*', '[', ']']: t_name = t_name.replace(char, '')\n",
        "    t_df = out_df[out_df['Teacher'] == t]\n",
        "    if not t_df.empty:\n",
        "        indiv_path = os.path.join(output_dir, f\"{t_name}_MTSS.xlsx\")\n",
        "        t_dist_df = generate_tier_distribution(t_df)\n",
        "\n",
        "        with pd.ExcelWriter(indiv_path, engine='openpyxl') as ind_writer:\n",
        "            t_df.to_excel(ind_writer, sheet_name=t_name, index=False)\n",
        "            t_dist_df.to_excel(ind_writer, sheet_name=teacher_dist_sheet_name, index=False)\n",
        "\n",
        "        ind_wb = openpyxl.load_workbook(indiv_path)\n",
        "        format_worksheet(ind_wb[t_name], list(out_df.columns))\n",
        "        format_worksheet(ind_wb[teacher_dist_sheet_name], list(t_dist_df.columns))\n",
        "        ind_wb.save(indiv_path)\n",
        "\n",
        "print(f\"[{SUBJECT_NAME}] Excel Packets Generated Successfully in OUTPUTS folder!\")"
      ],
      "metadata": {
        "id": "export_excel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca15e735-bf0c-492f-8591-2e9614adc14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Biology] Excel Packets Generated Successfully in OUTPUTS folder!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 8. Statistical Comparative Analysis\n",
        "# ==========================================\n",
        "class PDFReport(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font('Arial', 'B', 14)\n",
        "        self.cell(0, 10, f'{SCHOOL_NAME} {SUBJECT_NAME} Teacher Comparative Analysis', 0, 1, 'C')\n",
        "        self.set_font('Arial', 'I', 10)\n",
        "        self.cell(0, 8, 'Comparison of Average USA Scores Across Teachers', 0, 1, 'C')\n",
        "        self.ln(5)\n",
        "\n",
        "valid_data = out_df.dropna(subset=['Average_USA', 'Teacher'])\n",
        "teacher_counts = valid_data['Teacher'].value_counts()\n",
        "valid_teachers = teacher_counts[teacher_counts >= 3].index\n",
        "stat_df = valid_data[valid_data['Teacher'].isin(valid_teachers)]\n",
        "\n",
        "groups = [group['Average_USA'].values for name, group in stat_df.groupby('Teacher')]\n",
        "teacher_names = [name for name, group in stat_df.groupby('Teacher')]\n",
        "\n",
        "pdf = PDFReport()\n",
        "pdf.add_page()\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "pdf.set_font('Arial', 'B', 12)\n",
        "pdf.cell(0, 10, '1. Assumptions Testing', 0, 1)\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "# --- Shapiro-Wilk ---\n",
        "pdf.set_fill_color(240, 240, 240)\n",
        "pdf.multi_cell(0, 5, \"A. Normality Test (Shapiro-Wilk)\\n\"\n",
        "                     \"H0: The USA scores for a given teacher follow a normal distribution.\\n\"\n",
        "                     \"HA: The USA scores do NOT follow a normal distribution.\\n\"\n",
        "                     \"Course of Action: If H0 is rejected (p < 0.05) for ANY teacher, the assumption of normality \"\n",
        "                     \"is violated. We must abandon ANOVA and proceed with a Non-Parametric test.\", fill=True)\n",
        "pdf.ln(2)\n",
        "\n",
        "all_normal = True\n",
        "for name, group in zip(teacher_names, groups):\n",
        "    stat_val, p_val = stats.shapiro(group)\n",
        "    is_normal = p_val >= 0.05\n",
        "    if not is_normal: all_normal = False\n",
        "    mark = \"v\" if is_normal else \"X\"\n",
        "    pdf.multi_cell(0, 5, f\"   [{mark}] {name} (n={len(group)}): W={stat_val:.3f}, p={p_val:.3f} -> {'Normal' if is_normal else 'Not Normal'}\")\n",
        "\n",
        "pdf.set_font('Arial', 'B', 10)\n",
        "pdf.multi_cell(0, 6, f\"\\nOverall Normality met across all groups? {'Yes' if all_normal else 'No'}\")\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "# --- Levene's Test ---\n",
        "pdf.ln(4)\n",
        "pdf.multi_cell(0, 5, \"B. Homogeneity of Variances Test (Levene's Test)\\n\"\n",
        "                     \"H0: The population variances of USA scores are perfectly equal across all teachers.\\n\"\n",
        "                     \"HA: The population variances of USA scores are NOT equal.\\n\"\n",
        "                     \"Course of Action: If H0 is rejected (p < 0.05), the assumption of equal variance \"\n",
        "                     \"is violated. We must abandon ANOVA and proceed with a Non-Parametric test.\", fill=True)\n",
        "pdf.ln(2)\n",
        "\n",
        "stat_l, p_l = stats.levene(*groups)\n",
        "equal_var = p_l >= 0.05\n",
        "pdf.set_font('Arial', 'B', 10)\n",
        "pdf.multi_cell(0, 6, f\"Result: W={stat_l:.3f}, p={p_l:.3f} -> {'Equal Variances (H0 Accepted)' if equal_var else 'Unequal Variances (H0 Rejected)'}\")\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "# ==========================================\n",
        "# 2. Main Comparative Test\n",
        "# ==========================================\n",
        "pdf.ln(6)\n",
        "pdf.set_font('Arial', 'B', 12)\n",
        "pdf.cell(0, 10, '2. Main Comparative Test', 0, 1)\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "is_significant = False\n",
        "test_used = \"\"\n",
        "\n",
        "if all_normal and equal_var:\n",
        "    pdf.multi_cell(0, 5, \"Decision: Both assumptions met. Proceeding with Parametric One-Way ANOVA.\\n\"\n",
        "                         \"H0: The true mean USA scores are exactly equal across all teachers.\\n\"\n",
        "                         \"HA: At least one teacher's mean USA score is statistically significantly different.\\n\"\n",
        "                         \"Course of Action: If H0 is rejected (p < 0.05), we will run Tukey's HSD Post-Hoc \"\n",
        "                         \"test to determine exactly WHICH teachers differ from one another.\", fill=True)\n",
        "    pdf.ln(2)\n",
        "    f_stat, p_main = stats.f_oneway(*groups)\n",
        "    pdf.set_font('Arial', 'B', 10)\n",
        "    pdf.multi_cell(0, 6, f\"Result: F={f_stat:.3f}, p={p_main:.4f}\")\n",
        "    pdf.set_font('Arial', '', 10)\n",
        "    is_significant = p_main < 0.05\n",
        "    test_used = 'ANOVA'\n",
        "else:\n",
        "    pdf.multi_cell(0, 5, \"Decision: At least one assumption violated. Proceeding with Non-Parametric Kruskal-Wallis H-Test.\\n\"\n",
        "                         \"H0: The true median USA scores are exactly equal across all teachers.\\n\"\n",
        "                         \"HA: At least one teacher's median USA score is statistically significantly different.\\n\"\n",
        "                         \"Course of Action: If H0 is rejected (p < 0.05), we will run Pairwise Mann-Whitney U \"\n",
        "                         \"Tests with a strict Bonferroni Correction to determine exactly WHICH teachers differ.\", fill=True)\n",
        "    pdf.ln(2)\n",
        "    h_stat, p_main = stats.kruskal(*groups)\n",
        "    pdf.set_font('Arial', 'B', 10)\n",
        "    pdf.multi_cell(0, 6, f\"Result: H={h_stat:.3f}, p={p_main:.4f}\")\n",
        "    pdf.set_font('Arial', '', 10)\n",
        "    is_significant = p_main < 0.05\n",
        "    test_used = 'Kruskal'\n",
        "\n",
        "pdf.ln(2)\n",
        "pdf.set_font('Arial', 'I', 10)\n",
        "pdf.multi_cell(0, 6, f\">> Conclusion: {'Significant differences found (Proceed to Post-Hoc).' if is_significant else 'No significant differences found across teachers. Analysis stops here.'}\")\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Post-Hoc Analysis\n",
        "# ==========================================\n",
        "pdf.ln(6)\n",
        "pdf.set_font('Arial', 'B', 12)\n",
        "pdf.cell(0, 10, '3. Post-Hoc Analysis (Transparent Tables)', 0, 1)\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "if is_significant:\n",
        "    if test_used == 'ANOVA':\n",
        "        pdf.set_font('Arial', 'B', 11)\n",
        "        pdf.cell(0, 8, \"Test: Pairwise T-Tests & Tukey's HSD\", 0, 1)\n",
        "        pdf.set_font('Arial', '', 10)\n",
        "\n",
        "        pdf.multi_cell(0, 5, \"Showing both unadjusted T-Test p-values and Adjusted Tukey p-values for full transparency.\\n\")\n",
        "\n",
        "        tukey = pairwise_tukeyhsd(endog=stat_df['Average_USA'], groups=stat_df['Teacher'], alpha=0.05)\n",
        "        tukey_res = pd.DataFrame(data=tukey._results_table.data[1:], columns=tukey._results_table.data[0])\n",
        "\n",
        "        # Print Transparent Data Table\n",
        "        pdf.set_font('Courier', 'B', 8)\n",
        "        header = f\"{'Comparison':<45} | {'Unadj. p':<12} | {'Tukey p-adj':<12} | {'Significant?'}\"\n",
        "        pdf.cell(0, 5, header, 0, 1)\n",
        "        pdf.cell(0, 5, \"-\"*90, 0, 1)\n",
        "\n",
        "        pdf.set_font('Courier', '', 8)\n",
        "        for (i, name1), (j, name2) in itertools.combinations(enumerate(teacher_names), 2):\n",
        "            g1, g2 = groups[i], groups[j]\n",
        "            _, p_ttest = stats.ttest_ind(g1, g2)\n",
        "            row_tukey = tukey_res[((tukey_res['group1'] == name1) & (tukey_res['group2'] == name2)) |\n",
        "                                  ((tukey_res['group1'] == name2) & (tukey_res['group2'] == name1))]\n",
        "            p_adj = row_tukey['p-adj'].values[0] if not row_tukey.empty else 1.0\n",
        "            is_sig = \"Yes\" if p_adj < 0.05 else \"No\"\n",
        "\n",
        "            comp_name = f\"{name1[:20]} vs {name2[:20]}\"\n",
        "            row_str = f\"{comp_name:<45} | {p_ttest:<12.4f} | {p_adj:<12.4f} | {is_sig}\"\n",
        "            pdf.cell(0, 5, row_str, 0, 1)\n",
        "\n",
        "    elif test_used == 'Kruskal':\n",
        "        pdf.set_font('Arial', 'B', 11)\n",
        "        pdf.cell(0, 8, \"Test: Pairwise Mann-Whitney U Test (with Bonferroni Correction)\", 0, 1)\n",
        "        pdf.set_font('Arial', '', 10)\n",
        "\n",
        "        n_comparisons = len(teacher_names) * (len(teacher_names) - 1) / 2\n",
        "        alpha_corrected = 0.05 / n_comparisons\n",
        "\n",
        "        pdf.multi_cell(0, 5, f\"To avoid false positives from running multiple tests, the threshold for significance \"\n",
        "                             f\"(alpha=0.05) has been mathematically adjusted. Showing both unadjusted and adjusted values.\\n\"\n",
        "                             f\"Strict Adjusted Threshold (Bonferroni) = {alpha_corrected:.4f}\\n\")\n",
        "\n",
        "        # Print Transparent Data Table\n",
        "        pdf.set_font('Courier', 'B', 8)\n",
        "        header = f\"{'Comparison':<45} | {'Unadj. p':<12} | {'Adj. p':<12} | {'Significant?'}\"\n",
        "        pdf.cell(0, 5, header, 0, 1)\n",
        "        pdf.cell(0, 5, \"-\"*90, 0, 1)\n",
        "\n",
        "        pdf.set_font('Courier', '', 8)\n",
        "        for (i, name1), (j, name2) in itertools.combinations(enumerate(teacher_names), 2):\n",
        "            g1, g2 = groups[i], groups[j]\n",
        "            stat_mw, p_mw = stats.mannwhitneyu(g1, g2, alternative='two-sided')\n",
        "            adj_p = min(1.0, p_mw * n_comparisons)\n",
        "            is_sig = \"Yes\" if adj_p < 0.05 else \"No\"\n",
        "\n",
        "            comp_name = f\"{name1[:20]} vs {name2[:20]}\"\n",
        "            row_str = f\"{comp_name:<45} | {p_mw:<12.4f} | {adj_p:<12.4f} | {is_sig}\"\n",
        "            pdf.cell(0, 5, row_str, 0, 1)\n",
        "\n",
        "else:\n",
        "    pdf.multi_cell(0, 6, \"Skipping Post-Hoc Analysis because the main test found no significant overall differences.\")\n",
        "\n",
        "pdf_path = os.path.join(output_dir, f\"{SUBJECT_NAME}_Teacher_Comparative_Analysis.pdf\")\n",
        "pdf.output(pdf_path)\n",
        "print(f\"[{SUBJECT_NAME}] Statistical Report exported successfully in OUTPUTS folder!\")"
      ],
      "metadata": {
        "id": "statistical_analysis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d43c0c-4d93-4c38-fbc4-b1ac634232d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Biology] Statistical Report exported successfully in OUTPUTS folder!\n"
          ]
        }
      ]
    }
  ]
}