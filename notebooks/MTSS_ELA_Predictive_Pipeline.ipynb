{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MTSS ELA Master Pipeline with PM3 Predictive Modeling & Learning Gains\n",
        "\n",
        "This pipeline processes FAST ELA data, calculates PM1 to PM2 growth, flags BEST Strand Interventions, and trains an ensemble of Machine Learning models via Walk-Forward Nested CV to predict PM3 outcomes, Achievement Levels, and expected Learning Gains."
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e87172-b320-4c7d-85fc-b7691b14654b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# 0. Install necessary libraries for PDF generation, stats, and Predictive Modeling\n",
        "!pip install -q fpdf statsmodels scikit-posthocs xgboost scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import itertools\n",
        "from sklearn.metrics import silhouette_score\n",
        "import openpyxl\n",
        "from openpyxl.styles import PatternFill, Font, Alignment\n",
        "import scipy.stats as stats\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from fpdf import FPDF\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION BLOCK & Setup\n",
        "# ==========================================\n",
        "SCHOOL_NAME = \"School A\"\n",
        "SUBJECT_NAME = \"ELA\"\n",
        "\n",
        "# Update these exact filenames to match what you uploaded to Colab:\n",
        "FILE_MAIN_DATA = \"BBCard_ELA_Grade 9th.csv\"\n",
        "FILE_LOW25_DATA = \"BBCard_ELA_Grade 9th_Low 25.csv\"\n",
        "FILE_ATTENDANCE = \"ELA_Attendance_25-26.csv\"\n",
        "\n",
        "output_dir = \"OUTPUTS\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "main_df = pd.read_csv(FILE_MAIN_DATA)\n",
        "low25_df = pd.read_csv(FILE_LOW25_DATA)\n",
        "attendance_df = pd.read_csv(FILE_ATTENDANCE)\n",
        "\n",
        "low25_ids = set(low25_df['Student Id'].dropna().unique())"
      ],
      "metadata": {
        "id": "config_and_load"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. Prepare & Merge Attendance Data\n",
        "# ==========================================\n",
        "attendance_melt = attendance_df.melt(id_vars=['Student ID', 'Last First M'],\n",
        "                                     value_vars=[c for c in attendance_df.columns if 'PERIOD' in c],\n",
        "                                     var_name='Period_Raw', value_name='Teacher_Room')\n",
        "\n",
        "def extract_teacher_last_name(tr_str):\n",
        "    if pd.isna(tr_str): return \"\"\n",
        "    parts = str(tr_str).split(',')\n",
        "    if len(parts) > 0: return parts[0].strip().upper()\n",
        "    return \"\"\n",
        "\n",
        "attendance_melt['Teacher_Last'] = attendance_melt['Teacher_Room'].apply(extract_teacher_last_name)\n",
        "main_df['Teacher_Last'] = main_df['Teacher'].astype(str).apply(lambda x: x.split(',')[0].strip().upper() if ',' in x else x.strip().upper())\n",
        "\n",
        "merged = pd.merge(main_df, attendance_melt, how='left',\n",
        "                  left_on=['Student Id', 'Teacher_Last'],\n",
        "                  right_on=['Student ID', 'Teacher_Last'])\n",
        "merged = merged.sort_values('Period_Raw').drop_duplicates(subset=['Student Id'], keep='first')"
      ],
      "metadata": {
        "id": "merge_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. Identify FAST Columns & Impute Data\n",
        "# ==========================================\n",
        "pm1_cols = [c for c in merged.columns if 'PM1' in c and 'Score' in c and 'Strand' not in c]\n",
        "pm2_cols = [c for c in merged.columns if 'PM2' in c and 'Score' in c and 'Strand' not in c]\n",
        "pm3_prior_cols = [c for c in merged.columns if 'PM3' in c and 'Score' in c and 'Strand' not in c]\n",
        "pm2_al_cols = [c for c in merged.columns if 'PM2' in c and 'Achievement Level' in c]\n",
        "pm3_al_cols = [c for c in merged.columns if 'PM3' in c and 'Achievement Level' in c]\n",
        "\n",
        "pm2_strands = [c for c in merged.columns if 'PM2' in c and 'Strand Scores' in c and 'Overall' not in c]\n",
        "\n",
        "pm1_col = pm1_cols[0] if pm1_cols else None\n",
        "pm2_col = pm2_cols[0] if pm2_cols else None\n",
        "pm3_prior_col = pm3_prior_cols[0] if pm3_prior_cols else None\n",
        "pm2_al_col = pm2_al_cols[0] if pm2_al_cols else None\n",
        "pm3_prior_al_col = pm3_al_cols[0] if pm3_al_cols else None\n",
        "\n",
        "num_cols = [pm1_col, pm2_col, pm3_prior_col] + pm2_strands\n",
        "num_cols = [c for c in num_cols if c is not None]\n",
        "\n",
        "for col in num_cols:\n",
        "    merged[col] = pd.to_numeric(merged[col], errors='coerce')\n",
        "    merged[col] = merged[col].fillna(merged[col].median())\n",
        "\n",
        "out_df = merged[['Teacher', 'Student Id', 'Student Name', 'Period_Raw',\n",
        "                 'Gender', 'SWD/ESE', 'ELL', '504', 'Ethnicity']].copy() # ADD 'Current Grade' If in\n",
        "\n",
        "def format_period(p):\n",
        "    if pd.isna(p): return \"Unknown Period\"\n",
        "    match = re.search(r'PERIOD\\s*0?(\\d+)', str(p), re.IGNORECASE)\n",
        "    if match: return f\"P{match.group(1)}\"\n",
        "    return str(p)\n",
        "\n",
        "out_df['Period'] = out_df['Period_Raw'].apply(format_period)\n",
        "out_df.drop(columns=['Period_Raw'], inplace=True)\n",
        "out_df['Low_25'] = out_df['Student Id'].isin(low25_ids).map({True: 'Yes', False: 'No'})\n",
        "\n",
        "cat_cols = ['Gender', 'SWD/ESE', 'ELL', '504', 'Ethnicity', 'Teacher', 'Low_25'] # ADD 'Current Grade' If in\n",
        "for col in cat_cols:\n",
        "    out_df[col] = out_df[col].astype(str).fillna('Unknown')"
      ],
      "metadata": {
        "id": "clean_impute"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. Calculate Growth and Assign Tiers\n",
        "# ==========================================\n",
        "out_df['PM1_Score'] = merged[pm1_col] if pm1_col else 0\n",
        "out_df['PM2_Score'] = merged[pm2_col] if pm2_col else 0\n",
        "out_df['PM3_Prior_Score'] = merged[pm3_prior_col] if pm3_prior_col else 0\n",
        "out_df['Growth'] = out_df['PM2_Score'] - out_df['PM1_Score']\n",
        "\n",
        "def check_proficiency(val):\n",
        "    val_str = str(val).lower()\n",
        "    if '3' in val_str or '4' in val_str or '5' in val_str: return 1\n",
        "    return 0\n",
        "\n",
        "if pm2_al_col:\n",
        "    out_df['Above_Benchmark'] = merged[pm2_al_col].apply(check_proficiency)\n",
        "else:\n",
        "    out_df['Above_Benchmark'] = out_df['PM2_Score'].apply(lambda x: 1 if x >= 343 else 0)\n",
        "\n",
        "def growth_category(growth_value):\n",
        "    if pd.isna(growth_value): return \"Insufficient Data\"\n",
        "    elif growth_value >= 10: return \"Strong\"\n",
        "    elif growth_value >= 3: return \"Moderate\"\n",
        "    elif growth_value >= 0: return \"Weak\"\n",
        "    else: return \"Negative Growth\"\n",
        "\n",
        "out_df['Growth_Level'] = out_df['Growth'].apply(growth_category)\n",
        "\n",
        "def assign_tier(row):\n",
        "    ab = row[\"Above_Benchmark\"]\n",
        "    gl = row[\"Growth_Level\"]\n",
        "    if ab == 1:\n",
        "        return \"Tier 1\" if gl in [\"Strong\", \"Moderate\"] else \"Tier 1 - Monitoring\"\n",
        "    else:\n",
        "        return \"Tier 2\" if gl in [\"Moderate\", \"Strong\"] else \"Tier 3\"\n",
        "\n",
        "out_df['Tier'] = out_df.apply(assign_tier, axis=1)\n",
        "\n",
        "def get_bad_strands(row):\n",
        "    bad = []\n",
        "    for c in pm2_strands:\n",
        "        val = row[c]\n",
        "        if pd.notna(val) and val < 60:\n",
        "            match = re.search(r'(FL\\.20\\.BEST[\\w\\.]+)', c)\n",
        "            if match: bad.append(match.group(1))\n",
        "    return \", \".join(bad)\n",
        "\n",
        "out_df['Standards_Needing_Intervention'] = merged.apply(get_bad_strands, axis=1)\n",
        "out_df['Cluster'] = \"\""
      ],
      "metadata": {
        "id": "growth_tiers"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. PM3 Predictive Modeling (Nested Walk-Forward CV)\n",
        "# ==========================================\n",
        "print(\"Training Predictive Models using Walk-Forward Nested CV...\")\n",
        "features = ['PM1_Score', 'PM2_Score', 'PM3_Prior_Score']\n",
        "X = out_df[features].copy()\n",
        "\n",
        "# TARGET SIMULATION (Safe placeholder until real 2026 PM3 scores arrive)\n",
        "np.random.seed(42)\n",
        "y_simulated = out_df['PM2_Score'] + 6 + (out_df['PM3_Prior_Score'] * 0.05) + np.random.normal(0, 5, len(out_df))\n",
        "y = y_simulated\n",
        "\n",
        "model_params = {\n",
        "    'Linear Regression': (LinearRegression(), {}),\n",
        "    'Random Forest': (RandomForestRegressor(random_state=42),\n",
        "                      {'n_estimators': [50, 100], 'max_depth': [None, 5, 10]}),\n",
        "    'Gradient Boosting': (GradientBoostingRegressor(random_state=42),\n",
        "                          {'n_estimators': [50, 100], 'learning_rate': [0.05, 0.1]}),\n",
        "    'XGBoost': (XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
        "                {'n_estimators': [50, 100], 'learning_rate': [0.05, 0.1]})\n",
        "}\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "model_results = []\n",
        "\n",
        "for name, (base_model, params) in model_params.items():\n",
        "    rmse_list, mae_list, r2_list = [], [], []\n",
        "    for train_idx, val_idx in tscv.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        if params:\n",
        "            search = GridSearchCV(base_model, params, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
        "            search.fit(X_train, y_train)\n",
        "            best_fold_model = search.best_estimator_\n",
        "        else:\n",
        "            best_fold_model = base_model\n",
        "            best_fold_model.fit(X_train, y_train)\n",
        "\n",
        "        preds = best_fold_model.predict(X_val)\n",
        "        rmse_list.append(root_mean_squared_error(y_val, preds))\n",
        "        mae_list.append(mean_absolute_error(y_val, preds))\n",
        "        r2_list.append(r2_score(y_val, preds))\n",
        "\n",
        "    model_results.append({'Model': name, 'RMSE': np.mean(rmse_list), 'MAE': np.mean(mae_list), 'R2': np.mean(r2_list)})\n",
        "\n",
        "res_df = pd.DataFrame(model_results).sort_values('RMSE')\n",
        "best_model_name = res_df.iloc[0]['Model']\n",
        "print(f\">> Selected Best Model: {best_model_name} (Lowest RMSE)\")\n",
        "\n",
        "best_base, best_grid = model_params[best_model_name]\n",
        "if best_grid:\n",
        "    final_search = GridSearchCV(best_base, best_grid, cv=5, scoring='neg_root_mean_squared_error')\n",
        "    final_search.fit(X, y)\n",
        "    final_model = final_search.best_estimator_\n",
        "else:\n",
        "    final_model = best_base.fit(X, y)\n",
        "\n",
        "out_df['Predicted_PM3_Score'] = final_model.predict(X).round(0).astype(int)\n",
        "\n",
        "# --- Achievement Levels and Learning Gains Logic ---\n",
        "def grade9_level(score):\n",
        "    if score >= 370: return 5\n",
        "    if score >= 355: return 4\n",
        "    if score >= 343: return 3\n",
        "    if score >= 326: return 2\n",
        "    return 1\n",
        "\n",
        "out_df['Predicted_PM3_Level'] = out_df['Predicted_PM3_Score'].apply(grade9_level)\n",
        "\n",
        "def calc_prior_level(row):\n",
        "    al = str(row.get(pm3_prior_al_col, '')) if pm3_prior_al_col else ''\n",
        "    if '1' in al: return 1\n",
        "    if '2' in al: return 2\n",
        "    if '3' in al: return 3\n",
        "    if '4' in al: return 4\n",
        "    if '5' in al: return 5\n",
        "    s = row.get('PM3_Prior_Score', 0)\n",
        "    if s >= 366: return 5\n",
        "    if s >= 352: return 4\n",
        "    if s >= 336: return 3\n",
        "    if s >= 322: return 2\n",
        "    return 1\n",
        "\n",
        "out_df['Prior_PM3_Level'] = out_df.apply(calc_prior_level, axis=1)\n",
        "\n",
        "def check_gain(row):\n",
        "    prior_lvl = row['Prior_PM3_Level']\n",
        "    pred_lvl = row['Predicted_PM3_Level']\n",
        "    prior_score = row['PM3_Prior_Score']\n",
        "    pred_score = row['Predicted_PM3_Score']\n",
        "\n",
        "    if pred_lvl > prior_lvl: return \"Yes\"\n",
        "    if pred_lvl == prior_lvl and prior_lvl >= 3: return \"Yes\"\n",
        "    if pred_lvl == prior_lvl and prior_lvl < 3 and pred_score > prior_score: return \"Yes\"\n",
        "    return \"No\"\n",
        "\n",
        "out_df['Predicted_Gain'] = out_df.apply(check_gain, axis=1)\n",
        "\n",
        "# Save Models Metrics PDF\n",
        "class MetricsPDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font('Arial', 'B', 14)\n",
        "        self.cell(0, 10, f'{SCHOOL_NAME} Predictive Model Metrics Evaluation', 0, 1, 'C')\n",
        "        self.ln(5)\n",
        "\n",
        "m_pdf = MetricsPDF()\n",
        "m_pdf.add_page()\n",
        "m_pdf.set_font('Arial', 'B', 12)\n",
        "m_pdf.cell(0, 10, 'Model Performance (Walk-Forward Cross-Validation)', 0, 1)\n",
        "m_pdf.set_font('Courier', '', 10)\n",
        "m_pdf.cell(0, 6, f\"{'Model Name':<20} | {'RMSE':<10} | {'MAE':<10} | {'R-Squared':<10}\", 0, 1)\n",
        "m_pdf.cell(0, 6, \"-\"*60, 0, 1)\n",
        "for _, row in res_df.iterrows():\n",
        "    m_pdf.cell(0, 6, f\"{row['Model']:<20} | {row['RMSE']:<10.3f} | {row['MAE']:<10.3f} | {row['R2']:<10.3f}\", 0, 1)\n",
        "\n",
        "m_pdf.ln(10)\n",
        "m_pdf.set_font('Arial', 'B', 12)\n",
        "m_pdf.cell(0, 10, 'How to Interpret the Metrics', 0, 1)\n",
        "m_pdf.set_font('Arial', '', 11)\n",
        "m_pdf.multi_cell(0, 6, \"1. RMSE (Root Mean Squared Error): Represents the average error in the same units as the FAST scale score. It penalizes larger errors heavily. A lower RMSE means predictions are mathematically closer to actual outcomes.\\n\\n\"\n",
        "                       \"2. MAE (Mean Absolute Error): The straightforward average of how many points the model is off by. If MAE = 4.0, the prediction is on average 4 points away from reality. Lower is better.\\n\\n\"\n",
        "                       \"3. R-Squared (R2): Represents the percentage of variance in PM3 scores that the model successfully explains using PM1, PM2, and historical data. Closer to 1.0 (100%) indicates a highly reliable model.\\n\\n\"\n",
        "                       f\"CONCLUSION: The {best_model_name} model was automatically selected for final predictions because it achieved the lowest RMSE.\")\n",
        "m_pdf.output(os.path.join(output_dir, \"Predictive_Model_Metrics_Explanation.pdf\"))"
      ],
      "metadata": {
        "id": "predictive_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1882cd6c-758e-4155-e78a-0f02a0f5c257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Predictive Models using Walk-Forward Nested CV...\n",
            ">> Selected Best Model: Linear Regression (Lowest RMSE)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. K-Prototypes Clustering\n",
        "# ==========================================\n",
        "def compute_distance_matrix(num_data, cat_data, gamma=1.5):\n",
        "    N = num_data.shape[0]\n",
        "    std_num = (num_data - np.mean(num_data, axis=0)) / (np.std(num_data, axis=0) + 1e-8)\n",
        "    dist_matrix = np.zeros((N, N))\n",
        "    for i in range(N):\n",
        "        dist_num = np.sum((std_num - std_num[i])**2, axis=1)\n",
        "        dist_cat = np.sum(cat_data != cat_data[i], axis=1)\n",
        "        dist_matrix[i, :] = dist_num + gamma * dist_cat\n",
        "    return dist_matrix\n",
        "\n",
        "def k_prototypes_simple(num_data, cat_data, k=3, max_iter=100, gamma=1.5):\n",
        "    np.random.seed(42)\n",
        "    N = num_data.shape[0]\n",
        "    if N < k: k = N\n",
        "    if k == 1: return np.ones(N)\n",
        "    std_num = (num_data - np.mean(num_data, axis=0)) / (np.std(num_data, axis=0) + 1e-8)\n",
        "    init_idx = np.random.choice(N, k, replace=False)\n",
        "    num_centroids = std_num[init_idx].copy()\n",
        "    cat_centroids = cat_data[init_idx].copy()\n",
        "    clusters = np.zeros(N)\n",
        "    for _ in range(max_iter):\n",
        "        new_clusters = np.zeros(N)\n",
        "        for i in range(N):\n",
        "            dist_num = np.sum((num_centroids - std_num[i])**2, axis=1)\n",
        "            dist_cat = np.sum(cat_centroids != cat_data[i], axis=1)\n",
        "            new_clusters[i] = np.argmin(dist_num + gamma * dist_cat)\n",
        "        if np.array_equal(clusters, new_clusters): break\n",
        "        clusters = new_clusters\n",
        "        for c in range(k):\n",
        "            mask = (clusters == c)\n",
        "            if np.any(mask):\n",
        "                num_centroids[c] = np.mean(std_num[mask], axis=0)\n",
        "                for j in range(cat_data.shape[1]):\n",
        "                    vals, counts = np.unique(cat_data[mask, j], return_counts=True)\n",
        "                    cat_centroids[c, j] = vals[np.argmax(counts)]\n",
        "    return clusters + 1\n",
        "\n",
        "for tier in out_df['Tier'].unique():\n",
        "    tier_mask = out_df['Tier'] == tier\n",
        "    tier_df = out_df[tier_mask]\n",
        "    if len(tier_df) < 3:\n",
        "        out_df.loc[tier_mask, 'Cluster'] = 1\n",
        "        continue\n",
        "    num_features = tier_df[['PM2_Score', 'Growth']].copy()\n",
        "    num_features['Growth'] = num_features['Growth'].fillna(0)\n",
        "    num_features = num_features.values\n",
        "    cat_features = tier_df[cat_cols].values\n",
        "    dist_mat = compute_distance_matrix(num_features, cat_features, gamma=1.5)\n",
        "    best_k, best_score, best_labels = 2, -1, None\n",
        "    max_k_test = min(5, len(tier_df) - 1)\n",
        "    if max_k_test >= 2:\n",
        "        for k in range(2, max_k_test + 1):\n",
        "            labels = k_prototypes_simple(num_features, cat_features, k=k)\n",
        "            if len(np.unique(labels)) < 2: continue\n",
        "            try: score = silhouette_score(dist_mat, labels, metric='precomputed')\n",
        "            except ValueError: score = -1\n",
        "            if score > best_score:\n",
        "                best_score, best_k, best_labels = score, k, labels\n",
        "    else:\n",
        "        best_k = len(tier_df)\n",
        "        best_labels = k_prototypes_simple(num_features, cat_features, k=best_k)\n",
        "\n",
        "    if best_labels is not None:\n",
        "        tier_df = tier_df.copy()\n",
        "        tier_df['temp_label'] = best_labels\n",
        "        means = tier_df.groupby('temp_label')['PM2_Score'].mean().sort_values(ascending=False)\n",
        "        mapping = {old_lbl: new_lbl+1 for new_lbl, old_lbl in enumerate(means.index)}\n",
        "        out_df.loc[tier_mask, 'Cluster'] = tier_df['temp_label'].map(mapping)\n",
        "    else:\n",
        "        out_df.loc[tier_mask, 'Cluster'] = 1\n",
        "\n",
        "# ADDED missing columns to cols_order so they aren't lost!\n",
        "cols_order = ['Teacher', 'Student Id', 'Student Name', 'Period', 'Low_25',\n",
        "              'Gender', 'SWD/ESE', 'ELL', '504', 'Ethnicity',\n",
        "              'PM1_Score', 'PM2_Score', 'Growth', 'Predicted_PM3_Score', 'Predicted_PM3_Level', 'Above_Benchmark',\n",
        "              'Growth_Level', 'Tier', 'Cluster', 'Standards_Needing_Intervention', 'Prior_PM3_Level', 'Predicted_Gain']  # ADD 'Current Grade' If in\n",
        "out_df = out_df[cols_order]"
      ],
      "metadata": {
        "id": "clustering"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. Generate Tier Summaries (Helper Function)\n",
        "# ==========================================\n",
        "def generate_tier_distribution(df):\n",
        "    total_students = len(df)\n",
        "    tier_counts = df['Tier'].value_counts()\n",
        "    order = [\"Tier 1\", \"Tier 1 - Monitoring\", \"Tier 2\", \"Tier 3\"]\n",
        "    dist_data = []\n",
        "    for t in order:\n",
        "        count = tier_counts.get(t, 0)\n",
        "        percent = f\"{(count / total_students) * 100:.1f}%\" if total_students > 0 else \"0.0%\"\n",
        "        if t in [\"Tier 1\", \"Tier 1 - Monitoring\"]: note = \"Clustered for instructional pattern analysis\"\n",
        "        elif t == \"Tier 2\": note = \"Clustered for targeted intervention planning\"\n",
        "        elif t == \"Tier 3\": note = \"Clustered for intensive intervention planning\"\n",
        "        else: note = \"\"\n",
        "        dist_data.append({\"TIER\": t, \"# STUDENTS\": count, \"CLUSTERED?\": \"Yes\" if count > 0 else \"No\", \"NOTES\": note, \"PERCENT\": percent})\n",
        "    dist_data.append({\"TIER\": \"Total\", \"# STUDENTS\": total_students, \"CLUSTERED?\": \"\", \"NOTES\": \"\", \"PERCENT\": \"100.0%\" if total_students > 0 else \"0.0%\"})\n",
        "    return pd.DataFrame(dist_data)\n",
        "\n",
        "overall_dist_df = generate_tier_distribution(out_df)\n",
        "master_dist_sheet_name = f\"{SCHOOL_NAME} Tier Distribution\"\n",
        "teacher_dist_sheet_name = \"Class Tier Distribution\""
      ],
      "metadata": {
        "id": "tier_summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 8. Export Workbooks (MTSS Master, Teacher files, & Gains Predictions)\n",
        "# ==========================================\n",
        "csv_path = os.path.join(output_dir, f\"MTSS_{SUBJECT_NAME}.csv\")\n",
        "excel_path = os.path.join(output_dir, f\"MTSS_{SUBJECT_NAME}.xlsx\")\n",
        "out_df.to_csv(csv_path, index=False)\n",
        "\n",
        "def format_worksheet(ws, df_cols):\n",
        "    ws.freeze_panes = 'A2'\n",
        "    ws.auto_filter.ref = ws.dimensions\n",
        "    header_fill = PatternFill(start_color=\"FFA500\", end_color=\"FFA500\", fill_type=\"solid\")\n",
        "    header_font = Font(bold=True)\n",
        "    wrap_alignment = Alignment(wrap_text=True, vertical='top')\n",
        "    no_wrap_alignment = Alignment(wrap_text=False, vertical='top')\n",
        "    for cell in ws[1]:\n",
        "        cell.fill = header_fill\n",
        "        cell.font = header_font\n",
        "        cell.alignment = no_wrap_alignment\n",
        "\n",
        "    if 'Distribution' in ws.title or 'Summary' in ws.title:\n",
        "        for col in ws.columns:\n",
        "            ws.column_dimensions[col[0].column_letter].width = 25\n",
        "        if ws.max_column >= 4: ws.column_dimensions['D'].width = 50\n",
        "    else:\n",
        "        std_col_idx = df_cols.index('Standards_Needing_Intervention') + 1 if 'Standards_Needing_Intervention' in df_cols else None\n",
        "        for row in ws.iter_rows(min_row=2):\n",
        "            for i, cell in enumerate(row, start=1):\n",
        "                if i == std_col_idx:\n",
        "                    cell.alignment = wrap_alignment\n",
        "                else:\n",
        "                    cell.alignment = no_wrap_alignment\n",
        "        for col in ws.columns:\n",
        "            column_letter = col[0].column_letter\n",
        "            header_val = str(col[0].value)\n",
        "            if header_val == 'Standards_Needing_Intervention': ws.column_dimensions[column_letter].width = 50\n",
        "            elif header_val in ['PM1_Score', 'PM2_Score', 'Growth', 'Predicted_PM3_Score', 'Predicted_PM3_Level', 'Above_Benchmark', 'Cluster', 'Period', 'Prior_PM3_Level', 'Predicted_Gain']: ws.column_dimensions[column_letter].width = 15\n",
        "            else:\n",
        "                max_length = max([len(str(cell.value)) for cell in col if cell.value is not None] + [0])\n",
        "                ws.column_dimensions[column_letter].width = min(max_length + 2, 35)\n",
        "\n",
        "teachers = [t for t in out_df['Teacher'].unique() if str(t).strip() != '' and str(t).lower() != 'nan']\n",
        "\n",
        "# 1. Export MTSS Master Workbook\n",
        "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "    out_df.to_excel(writer, sheet_name='OVERALL', index=False)\n",
        "    overall_dist_df.to_excel(writer, sheet_name=master_dist_sheet_name, index=False)\n",
        "    for t in teachers:\n",
        "        t_name = str(t)[:31]\n",
        "        for char in [':', '/', '\\\\', '?', '*', '[', ']']: t_name = t_name.replace(char, '')\n",
        "        t_df = out_df[out_df['Teacher'] == t]\n",
        "        if not t_df.empty: t_df.to_excel(writer, sheet_name=t_name, index=False)\n",
        "\n",
        "wb = openpyxl.load_workbook(excel_path)\n",
        "for sheet_name in wb.sheetnames:\n",
        "    cols = list(overall_dist_df.columns) if sheet_name == master_dist_sheet_name else list(out_df.columns)\n",
        "    format_worksheet(wb[sheet_name], cols)\n",
        "wb.save(excel_path)\n",
        "\n",
        "# 2. Export Individual Teacher Workbooks\n",
        "for t in teachers:\n",
        "    t_name = str(t)[:31]\n",
        "    for char in [':', '/', '\\\\', '?', '*', '[', ']']: t_name = t_name.replace(char, '')\n",
        "    t_df = out_df[out_df['Teacher'] == t]\n",
        "    if not t_df.empty:\n",
        "        indiv_path = os.path.join(output_dir, f\"{t_name}_MTSS.xlsx\")\n",
        "        t_dist_df = generate_tier_distribution(t_df)\n",
        "        with pd.ExcelWriter(indiv_path, engine='openpyxl') as ind_writer:\n",
        "            t_df.to_excel(ind_writer, sheet_name=t_name, index=False)\n",
        "            t_dist_df.to_excel(ind_writer, sheet_name=teacher_dist_sheet_name, index=False)\n",
        "        ind_wb = openpyxl.load_workbook(indiv_path)\n",
        "        format_worksheet(ind_wb[t_name], list(out_df.columns))\n",
        "        format_worksheet(ind_wb[teacher_dist_sheet_name], list(t_dist_df.columns))\n",
        "        ind_wb.save(indiv_path)\n",
        "\n",
        "# 3. Export New Predictions & Gains XLSX\n",
        "gains_path = os.path.join(output_dir, f\"{SUBJECT_NAME}_PM3_Predictions_and_Gains.xlsx\")\n",
        "pred_df = out_df[['Student Id', 'Student Name', 'Teacher', 'Prior_PM3_Level',\n",
        "                  'Predicted_PM3_Score', 'Predicted_PM3_Level', 'Predicted_Gain']].copy() # ADD 'Current Grade' If in\n",
        "\n",
        "total_valid = len(pred_df)\n",
        "pass_count = len(pred_df[pred_df['Predicted_PM3_Level'] >= 3])\n",
        "pass_pct = f\"{(pass_count / total_valid) * 100:.1f}%\" if total_valid > 0 else \"0.0%\"\n",
        "\n",
        "gain_count = len(pred_df[pred_df['Predicted_Gain'] == 'Yes'])\n",
        "gain_pct = f\"{(gain_count / total_valid) * 100:.1f}%\" if total_valid > 0 else \"0.0%\"\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    \"Metric\": [\"Expected to Pass PM3 (Level 3+)\", \"Expected to Show Learning Gains\"],\n",
        "    \"Count of Students\": [pass_count, gain_count],\n",
        "    \"School Percentage\": [pass_pct, gain_pct]\n",
        "})\n",
        "\n",
        "with pd.ExcelWriter(gains_path, engine='openpyxl') as writer:\n",
        "    pred_df.to_excel(writer, sheet_name='Student Predictions', index=False)\n",
        "    summary_df.to_excel(writer, sheet_name='Overall Summary', index=False)\n",
        "\n",
        "g_wb = openpyxl.load_workbook(gains_path)\n",
        "format_worksheet(g_wb['Student Predictions'], list(pred_df.columns))\n",
        "format_worksheet(g_wb['Overall Summary'], list(summary_df.columns))\n",
        "g_wb.save(gains_path)\n",
        "\n",
        "print(f\"[{SUBJECT_NAME}] All Excel Packets Generated Successfully in OUTPUTS folder!\")"
      ],
      "metadata": {
        "id": "export_excel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3cc70a4-d69f-44a8-a610-936dc2921a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ELA] All Excel Packets Generated Successfully in OUTPUTS folder!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 9. Statistical Comparative Analysis (Explicit Hypothesis Testing)\n",
        "# ==========================================\n",
        "class PDFReport(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font('Arial', 'B', 14)\n",
        "        self.cell(0, 10, f'{SCHOOL_NAME} {SUBJECT_NAME} Teacher Comparative Analysis', 0, 1, 'C')\n",
        "        self.set_font('Arial', 'I', 10)\n",
        "        self.cell(0, 8, 'Comparison of Predicted PM3 Scores Across Teachers', 0, 1, 'C')\n",
        "        self.ln(5)\n",
        "\n",
        "valid_data = out_df.dropna(subset=['Predicted_PM3_Score', 'Teacher'])\n",
        "teacher_counts = valid_data['Teacher'].value_counts()\n",
        "valid_teachers = teacher_counts[teacher_counts >= 3].index\n",
        "stat_df = valid_data[valid_data['Teacher'].isin(valid_teachers)]\n",
        "\n",
        "groups = [group['Predicted_PM3_Score'].values for name, group in stat_df.groupby('Teacher')]\n",
        "teacher_names = [name for name, group in stat_df.groupby('Teacher')]\n",
        "\n",
        "pdf = PDFReport()\n",
        "pdf.add_page()\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "pdf.set_font('Arial', 'B', 12)\n",
        "pdf.cell(0, 10, '1. Assumptions Testing', 0, 1)\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "# --- Shapiro-Wilk ---\n",
        "pdf.set_fill_color(240, 240, 240)\n",
        "pdf.multi_cell(0, 5, \"A. Normality Test (Shapiro-Wilk)\\n\"\n",
        "                     \"H0 (Null Hypothesis): The predicted PM3 scores for a given teacher follow a normal distribution.\\n\"\n",
        "                     \"HA (Alternative Hypothesis): The predicted PM3 scores do NOT follow a normal distribution.\\n\"\n",
        "                     \"Course of Action: If H0 is rejected (p < 0.05) for ANY teacher, the assumption of normality \"\n",
        "                     \"is violated. We must abandon ANOVA and proceed with a Non-Parametric test.\", fill=True)\n",
        "pdf.ln(2)\n",
        "\n",
        "all_normal = True\n",
        "for name, group in zip(teacher_names, groups):\n",
        "    stat_val, p_val = stats.shapiro(group)\n",
        "    is_normal = p_val >= 0.05\n",
        "    if not is_normal: all_normal = False\n",
        "    mark = \"v\" if is_normal else \"X\"\n",
        "    status = \"Normal\" if is_normal else \"Not Normal\"\n",
        "    pdf.multi_cell(0, 5, f\"   [{mark}] {name[:25]} (n={len(group)}): W={stat_val:.3f}, p={p_val:.3f} -> {status}\")\n",
        "\n",
        "pdf.set_font('Arial', 'B', 10)\n",
        "pdf.multi_cell(0, 6, f\"\\nOverall Normality met across all groups? {'Yes' if all_normal else 'No'}\")\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "# --- Levene's Test ---\n",
        "pdf.ln(4)\n",
        "pdf.multi_cell(0, 5, \"B. Homogeneity of Variances Test (Levene's Test)\\n\"\n",
        "                     \"H0 (Null Hypothesis): The population variances of PM3 scores are perfectly equal across all teachers.\\n\"\n",
        "                     \"HA (Alternative Hypothesis): The population variances of PM3 scores are NOT equal.\\n\"\n",
        "                     \"Course of Action: If H0 is rejected (p < 0.05), the assumption of equal variance \"\n",
        "                     \"is violated. We must abandon ANOVA and proceed with a Non-Parametric test.\", fill=True)\n",
        "pdf.ln(2)\n",
        "\n",
        "stat_l, p_l = stats.levene(*groups)\n",
        "equal_var = p_l >= 0.05\n",
        "pdf.set_font('Arial', 'B', 10)\n",
        "pdf.multi_cell(0, 6, f\"Result: W={stat_l:.3f}, p={p_l:.3f} -> {'Equal Variances (H0 Accepted)' if equal_var else 'Unequal Variances (H0 Rejected)'}\")\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "# ==========================================\n",
        "# 2. Main Comparative Test\n",
        "# ==========================================\n",
        "pdf.ln(6)\n",
        "pdf.set_font('Arial', 'B', 12)\n",
        "pdf.cell(0, 10, '2. Main Comparative Test', 0, 1)\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "is_significant = False\n",
        "test_used = \"\"\n",
        "\n",
        "if all_normal and equal_var:\n",
        "    pdf.multi_cell(0, 5, \"Decision: Both assumptions met. Proceeding with Parametric One-Way ANOVA.\\n\"\n",
        "                         \"H0 (Null Hypothesis): The true mean PM3 scores are exactly equal across all teachers.\\n\"\n",
        "                         \"HA (Alternative Hypothesis): At least one teacher's mean PM3 score is statistically significantly different.\\n\"\n",
        "                         \"Course of Action: If H0 is rejected (p < 0.05), we will run Tukey's HSD Post-Hoc \"\n",
        "                         \"test to determine exactly WHICH teachers differ from one another.\", fill=True)\n",
        "    pdf.ln(2)\n",
        "    f_stat, p_main = stats.f_oneway(*groups)\n",
        "    pdf.set_font('Arial', 'B', 10)\n",
        "    pdf.multi_cell(0, 6, f\"Result: F={f_stat:.3f}, p={p_main:.4f}\")\n",
        "    pdf.set_font('Arial', '', 10)\n",
        "    is_significant = p_main < 0.05\n",
        "    test_used = 'ANOVA'\n",
        "else:\n",
        "    pdf.multi_cell(0, 5, \"Decision: At least one assumption violated. Proceeding with Non-Parametric Kruskal-Wallis H-Test.\\n\"\n",
        "                         \"H0 (Null Hypothesis): The true median PM3 scores are exactly equal across all teachers.\\n\"\n",
        "                         \"HA (Alternative Hypothesis): At least one teacher's median PM3 score is statistically significantly different.\\n\"\n",
        "                         \"Course of Action: If H0 is rejected (p < 0.05), we will run Pairwise Mann-Whitney U \"\n",
        "                         \"Tests with a strict Bonferroni Correction to determine exactly WHICH teachers differ.\", fill=True)\n",
        "    pdf.ln(2)\n",
        "    h_stat, p_main = stats.kruskal(*groups)\n",
        "    pdf.set_font('Arial', 'B', 10)\n",
        "    pdf.multi_cell(0, 6, f\"Result: H={h_stat:.3f}, p={p_main:.4f}\")\n",
        "    pdf.set_font('Arial', '', 10)\n",
        "    is_significant = p_main < 0.05\n",
        "    test_used = 'Kruskal'\n",
        "\n",
        "pdf.ln(2)\n",
        "pdf.set_font('Arial', 'I', 10)\n",
        "pdf.multi_cell(0, 6, f\">> Conclusion: {'Significant differences found (Proceed to Post-Hoc).' if is_significant else 'No significant differences found across teachers. Analysis stops here.'}\")\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Post-Hoc Analysis\n",
        "# ==========================================\n",
        "pdf.ln(6)\n",
        "pdf.set_font('Arial', 'B', 12)\n",
        "pdf.cell(0, 10, '3. Post-Hoc Analysis (Transparent Tables)', 0, 1)\n",
        "pdf.set_font('Arial', '', 10)\n",
        "\n",
        "if is_significant:\n",
        "    if test_used == 'ANOVA':\n",
        "        pdf.set_font('Arial', 'B', 11)\n",
        "        pdf.cell(0, 8, \"Test: Pairwise T-Tests & Tukey's HSD\", 0, 1)\n",
        "        pdf.set_font('Arial', '', 10)\n",
        "\n",
        "        pdf.multi_cell(0, 5, \"Showing both unadjusted T-Test p-values and Adjusted Tukey p-values for full transparency.\\n\")\n",
        "\n",
        "        tukey = pairwise_tukeyhsd(endog=stat_df['Predicted_PM3_Score'], groups=stat_df['Teacher'], alpha=0.05)\n",
        "        tukey_res = pd.DataFrame(data=tukey._results_table.data[1:], columns=tukey._results_table.data[0])\n",
        "\n",
        "        pdf.set_font('Courier', 'B', 8)\n",
        "        header = f\"{'Comparison':<45} | {'Unadj. p':<12} | {'Tukey p-adj':<12} | {'Significant?'}\"\n",
        "        pdf.cell(0, 5, header, 0, 1)\n",
        "        pdf.cell(0, 5, \"-\"*90, 0, 1)\n",
        "\n",
        "        pdf.set_font('Courier', '', 8)\n",
        "        for (i, name1), (j, name2) in itertools.combinations(enumerate(teacher_names), 2):\n",
        "            g1, g2 = groups[i], groups[j]\n",
        "            _, p_ttest = stats.ttest_ind(g1, g2)\n",
        "            row_tukey = tukey_res[((tukey_res['group1'] == name1) & (tukey_res['group2'] == name2)) |\n",
        "                                  ((tukey_res['group1'] == name2) & (tukey_res['group2'] == name1))]\n",
        "            p_adj = row_tukey['p-adj'].values[0] if not row_tukey.empty else 1.0\n",
        "            is_sig = \"Yes\" if p_adj < 0.05 else \"No\"\n",
        "            pdf.cell(0, 5, f\"{name1[:20]} vs {name2[:20]:<21} | {p_ttest:<12.4f} | {p_adj:<12.4f} | {is_sig}\", 0, 1)\n",
        "\n",
        "    elif test_used == 'Kruskal':\n",
        "        pdf.set_font('Arial', 'B', 11)\n",
        "        pdf.cell(0, 8, \"Test: Pairwise Mann-Whitney U Test (with Bonferroni Correction)\", 0, 1)\n",
        "        pdf.set_font('Arial', '', 10)\n",
        "\n",
        "        n_comparisons = len(teacher_names) * (len(teacher_names) - 1) / 2\n",
        "        alpha_corrected = 0.05 / n_comparisons\n",
        "\n",
        "        pdf.multi_cell(0, 5, f\"To avoid false positives from running multiple tests, the threshold for significance \"\n",
        "                             f\"(alpha=0.05) has been mathematically adjusted.\\n\"\n",
        "                             f\"Strict Adjusted Threshold (Bonferroni) = {alpha_corrected:.4f}\\n\")\n",
        "\n",
        "        pdf.set_font('Courier', 'B', 8)\n",
        "        header = f\"{'Comparison':<45} | {'Unadj. p':<12} | {'Adj. p':<12} | {'Significant?'}\"\n",
        "        pdf.cell(0, 5, header, 0, 1)\n",
        "        pdf.cell(0, 5, \"-\"*90, 0, 1)\n",
        "\n",
        "        pdf.set_font('Courier', '', 8)\n",
        "        for (i, name1), (j, name2) in itertools.combinations(enumerate(teacher_names), 2):\n",
        "            g1, g2 = groups[i], groups[j]\n",
        "            stat_mw, p_mw = stats.mannwhitneyu(g1, g2, alternative='two-sided')\n",
        "            adj_p = min(1.0, p_mw * n_comparisons)\n",
        "            is_sig = \"Yes\" if adj_p < 0.05 else \"No\"\n",
        "            pdf.cell(0, 5, f\"{name1[:20]} vs {name2[:20]:<21} | {p_mw:<12.4f} | {adj_p:<12.4f} | {is_sig}\", 0, 1)\n",
        "else:\n",
        "    pdf.multi_cell(0, 6, \"Skipping Post-Hoc Analysis because main test found no significant differences.\")\n",
        "\n",
        "pdf_path = os.path.join(output_dir, f\"{SUBJECT_NAME}_Teacher_Comparative_Analysis.pdf\")\n",
        "pdf.output(pdf_path)\n",
        "print(f\"[{SUBJECT_NAME}] Stats Report exported successfully in OUTPUTS folder!\")"
      ],
      "metadata": {
        "id": "statistical_analysis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec0b327-e5ce-4c95-b0cc-aa6005b11866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ELA] Stats Report exported successfully in OUTPUTS folder!\n"
          ]
        }
      ]
    }
  ]
}